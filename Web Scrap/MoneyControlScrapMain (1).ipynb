{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqgZrCf0NWUf",
        "outputId": "bdb38787-5278-4814-e416-964dae767d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install lxml\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_blog_url(soup):\n",
        "  div_=soup.find_all('div', attrs={'class': 'FL PR20'})\n",
        "  url_list=[]\n",
        "  for title in div_:\n",
        "    href =title.find('a')['href']\n",
        "    url_list.append(\"https://www.moneycontrol.com/\"+href)\n",
        "  return url_list"
      ],
      "metadata": {
        "id": "vAgFXixuNaf1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_blog_content(url):\n",
        "  request= requests.get(url)\n",
        "  soup =BeautifulSoup(request.text, 'lxml')\n",
        "  all_scripts= soup.find_all('script', attrs= {'type':'application/ld+json'})\n",
        "  raw_article_str=all_scripts[2].get_text().replace('\\r\\n','')\n",
        "  parts= re.split(r\"\"\"(\"[^\"]*\"|'[^']*')\"\"\", raw_article_str)\n",
        "  parts[::2] = map(lambda s: \"\".join(s.split()), parts[::2])\n",
        "  article_str=\"\".join(parts)\n",
        "  article_str= article_str[1:]\n",
        "  article_str= article_str[:-1]\n",
        "  article_dict= json.loads(article_str)\n",
        "  all_tags=soup.find_all('div', attrs= {'class':'tags_first_line'})\n",
        "  lst_all_tags=[]\n",
        "  for i in all_tags:\n",
        "    lst_all_tags.append(i.get_text())\n",
        "  tags= lst_all_tags[0].replace('TAGS:','')\n",
        "  tags=tags.replace('\\n','')\n",
        "  tags=tags.split('#')\n",
        "  tags= tags[1:]\n",
        "  tags=','.join([str(elem).strip() for elem in tags])\n",
        "  article_dict['tags']=tags\n",
        "  return article_dict"
      ],
      "metadata": {
        "id": "PdyhHnwbNhNs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_page_no(url, sc_id, page_no, next, year):\n",
        "  request= requests.get(url)\n",
        "  soup =BeautifulSoup(request.text, 'lxml')\n",
        "  all_page_no =soup.find_all('div', attrs={'class': 'pages MR10 MT15'})\n",
        "  page_list =[i.text for i in all_page_no[0].find_all('a')]\n",
        "  if any(map(str.isdigit, page_list[-1])):\n",
        "\n",
        "    return int(page_list[-1]), next\n",
        "\n",
        "  else:\n",
        "    next= next + 1\n",
        "    page_no =int(page_list[-2])\n",
        "    url=\"https://www.moneycontrol.com/stocks/company_info/stock_news.php?sc_id=\"+sc_id+\"&scat=&pageno=\"+str(page_no)+\"&next=\"+str(next)+\"&durationType=Y&Year=\"+str(year)+\"&duration=1&news_type=\"\n",
        "    return get_page_no(url, sc_id, page_no, next, year)"
      ],
      "metadata": {
        "id": "MBVBgGkQNh2s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_company_data(url_=\"https://www.moneycontrol.com/stocks/company_info/stock_news.php?\", sc_id=[], page_no=1, next=0, years=[]):\n",
        "  for company in sc_id:\n",
        "    df=pd.DataFrame(columns=['company', 'datePublished', 'author', 'headline', 'description', 'articleBody', 'tags', 'url'])\n",
        "    for year in years:\n",
        "      print('year:', year)\n",
        "      print('page_no:',page_no)\n",
        "      print('next:', next)\n",
        "      url=url_ + \"sc id=\"+company+\"&scat=&pageno=\"+str(page_no)+\"&next=\"+str(next)+\"&durationType=Y&Year=\"+str(year)+\"&duration=1&news_type=\"\n",
        "      print('url:', url)\n",
        "\n",
        "      max_page_no, max_next= get_page_no(url, company, page_no, next, year)\n",
        "      max_next= max_next + 1\n",
        "\n",
        "      for i in range(max_next):\n",
        "        for j in range((i*10)+1, (i*10)+11):\n",
        "          if j <= max_page_no:\n",
        "            url_list=[]\n",
        "            url=url_ + \"sc_id=\"+company+\"&scat=&pageno=\"+str(j)+\"&next=\"+str(i)+\"&durationType=Y&Year=\"+str(year)+\"&duration=1&news_type=\"\n",
        "            request= requests.get(url)\n",
        "            soup= BeautifulSoup(request.text, 'lxml')\n",
        "            url_list= get_blog_url(soup)\n",
        "\n",
        "            for url in url_list:\n",
        "              try:\n",
        "                article_dict= get_blog_content(url)\n",
        "                print(company)\n",
        "                print(article_dict['datePublished'])\n",
        "                print(article_dict['author'])\n",
        "                print(article_dict['headline'])\n",
        "                print(article_dict['description'])\n",
        "                print(article_dict['articleBody'])\n",
        "                print(article_dict['tags'])\n",
        "                print(article_dict['url'])\n",
        "                print('------------------------------------------')\n",
        "\n",
        "                article_lst =[[company,\n",
        "                              article_dict['datePublished'],\n",
        "                              article_dict['author'],\n",
        "                              article_dict['headline'],\n",
        "                              article_dict['description'],\n",
        "                              article_dict['articleBody'],\n",
        "                              article_dict['tags'], url]]\n",
        "\n",
        "\n",
        "                df = pd.concat([df, pd.DataFrame(article_lst, columns=['company', 'datePublished', 'author', 'headline', 'description', 'articleBody', 'tags', 'url'])], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "              except:\n",
        "\n",
        "                   article_lst =[[company, 'error', 'error', 'error', 'error', 'error', 'error', url]]\n",
        "\n",
        "                   df = pd.concat([df, pd.DataFrame(article_lst, columns=['company', 'datePublished', 'author', 'headline', 'description', 'articleBody', 'tags', 'url'])], ignore_index=True)\n",
        "\n",
        "                   continue\n",
        "          else:\n",
        "                 break\n",
        "  df.to_csv('/content/drive/MyDrive/ALLM_Project/2018'+company+'.csv')"
      ],
      "metadata": {
        "id": "znSF9Ie4NpvT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(save_company_data(sc_id=[\"RI\"], page_no=1, next=0, years=[2018]))"
      ],
      "metadata": {
        "id": "BRbeCql5Ns6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc_id=\"RI\"\n",
        "page_no=1\n",
        "next=0\n",
        "year=2017\n",
        "url=\"https://www.moneycontrol.com/stocks/company_info/stock_news.php?sc_id=\"+sc_id+\"&scat=&pageno=\"+str(page_no)+\"&next=\"+str(next)+\"&durationType=Y&Year=\"+str(year)+\"&duration=1&news_type=\"\n",
        "value4=get_page_no(url, sc_id, page_no, next, year)"
      ],
      "metadata": {
        "id": "rIj0jcErO1BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(value4)"
      ],
      "metadata": {
        "id": "kVULv8KYO1vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLwajH1rUjh5",
        "outputId": "0a8b96ce-5af9-436a-bbb5-6920e3a8b17d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_blog_url(soup):\n",
        "    div_ = soup.find_all('div', attrs={'class': 'FL PR20'})\n",
        "    url_list = []\n",
        "    for title in div_:\n",
        "        href = title.find('a')['href']\n",
        "        url_list.append(\"https://www.moneycontrol.com/\" + href)\n",
        "    return url_list"
      ],
      "metadata": {
        "id": "vsW5n2JrUkPq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_blog_content(url):\n",
        "    request = requests.get(url)\n",
        "    soup = BeautifulSoup(request.text, 'lxml')\n",
        "    all_scripts = soup.find_all('script', attrs={'type': 'application/ld+json'})\n",
        "    raw_article_str = all_scripts[2].get_text().replace('\\r\\n', '')\n",
        "    parts = re.split(r\"\"\"(\"[^\"]*\"|'[^']*')\"\"\", raw_article_str)\n",
        "    parts[::2] = map(lambda s: \"\".join(s.split()), parts[::2])\n",
        "    article_str = \"\".join(parts)\n",
        "    article_str = article_str[1:]\n",
        "    article_str = article_str[:-1]\n",
        "    article_dict = json.loads(article_str)\n",
        "    all_tags = soup.find_all('div', attrs={'class': 'tags_first_line'})\n",
        "    lst_all_tags = []\n",
        "    for i in all_tags:\n",
        "        lst_all_tags.append(i.get_text())\n",
        "    tags = lst_all_tags[0].replace('TAGS:', '')\n",
        "    tags = tags.replace('\\n', '')\n",
        "    tags = tags.split('#')\n",
        "    tags = ','.join([str(elem).strip() for elem in tags])\n",
        "    article_dict['tags'] = tags\n",
        "    return article_dict"
      ],
      "metadata": {
        "id": "EzrzvtXWUmlF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_page_no(url, sc_id, page_number, next_page, year):\n",
        "    request = requests.get(url)\n",
        "    soup = BeautifulSoup(request.text, 'lxml')\n",
        "    all_page_no = soup.find_all('div', attrs={'class': 'pages MR10 MT15'})\n",
        "    page_list = [i.text for i in all_page_no[0].find_all('a')]\n",
        "    if any(map(str.isdigit, page_list[-1])):\n",
        "        return int(page_list[-1]), next_page\n",
        "    else:\n",
        "        next_page += 1\n",
        "        page_number = int(page_list[-2])\n",
        "        url = f\"https://www.moneycontrol.com/stocks/company_info/stock_news.php?sc_id={sc_id}&scat=&pageno={page_number}&next={next_page}&durationType=Y&Year={year}&duration=1&news_type=\"\n",
        "        return get_page_no(url, sc_id, page_number, next_page, year)"
      ],
      "metadata": {
        "id": "YIWiOEWrUp69"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_page_no(url, sc_id, page_number, next_page, year):\n",
        "    request = requests.get(url)\n",
        "    soup = BeautifulSoup(request.text, 'lxml')\n",
        "    all_page_no = soup.find_all('div', attrs={'class': 'pages MR10 MT15'})\n",
        "    if all_page_no:\n",
        "        page_list = [i.text for i in all_page_no[0].find_all('a')]\n",
        "        if any(map(str.isdigit, page_list[-1])):\n",
        "            return int(page_list[-1]), next_page\n",
        "        else:\n",
        "            next_page += 1\n",
        "            page_number = int(page_list[-2])\n",
        "            url = f\"https://www.moneycontrol.com/stocks/company_info/stock_news.php?sc_id={sc_id}&scat=&pageno={page_number}&next={next_page}&durationType=Y&Year={year}&duration=1&news_type=\"\n",
        "            return get_page_no(url, sc_id, page_number, next_page, year)\n",
        "    else:\n",
        "        # Handle the case where the page navigation elements are not found\n",
        "        return 0, 0  # Return default values if navigation elements are not found\n"
      ],
      "metadata": {
        "id": "mjF231H2YL5B"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_company_data(url_=\"https://www.moneycontrol.com/stocks/company_info/stock_news.php?\", sc_id=[], page_number=1, next_page=0, years=[]):\n",
        "    all_data = []\n",
        "    for company in sc_id:\n",
        "        df = pd.DataFrame(columns=['company', 'datePublished', 'author', 'headline', 'description', 'articleBody', 'tags', 'url'])\n",
        "        for year in years:\n",
        "            print('year:', year)\n",
        "            print('page_number:', page_number)\n",
        "            print('next_page:', next_page)\n",
        "            url = f\"{url_}sc_id={company}&scat=&pageno={page_number}&next={next_page}&durationType=Y&Year={year}&duration=1&news_type=\"\n",
        "            print('url:', url)\n",
        "            max_page_no, max_next = get_page_no(url, company, page_number, next_page, year)\n",
        "            max_next += 1\n",
        "\n",
        "            for i in range(max_next):\n",
        "                for j in range((i * 10) + 1, (i * 10) + 11):\n",
        "                    if j <= max_page_no:\n",
        "                        url_list = []\n",
        "                        url = f\"{url_}sc_id={company}&scat=&pageno={j}&next={i}&durationType=Y&Year={year}&duration=1&news_type=\"\n",
        "                        request = requests.get(url)\n",
        "                        soup = BeautifulSoup(request.text, 'lxml')\n",
        "                        url_list = get_blog_url(soup)\n",
        "\n",
        "                        for url in url_list:\n",
        "                            try:\n",
        "                                article_dict = get_blog_content(url)\n",
        "                                print(company)\n",
        "                                print(article_dict['datePublished'])\n",
        "                                print(article_dict['author'])\n",
        "                                print(article_dict['headline'])\n",
        "                                print(article_dict['description'])\n",
        "                                print(article_dict['articleBody'])\n",
        "                                print(article_dict['tags'])\n",
        "                                print(article_dict['url'])\n",
        "                                print('------------------------------------------')\n",
        "\n",
        "                                article_lst = [[company,\n",
        "                                                article_dict['datePublished'],\n",
        "                                                article_dict['author'],\n",
        "                                                article_dict['headline'],\n",
        "                                                article_dict['description'],\n",
        "                                                article_dict['articleBody'],\n",
        "                                                article_dict['tags'], url]]\n",
        "\n",
        "                                df = pd.concat([df, pd.DataFrame(article_lst, columns=['company', 'datePublished', 'author', 'headline', 'description', 'articleBody', 'tags', 'url'])], ignore_index=True)\n",
        "\n",
        "                            except Exception as e:\n",
        "                                print(f\"Error occurred for {url}: {e}\")\n",
        "\n",
        "                    else:\n",
        "                        break\n",
        "        #all_data.append(df)\n",
        "    df.to_csv('/content/drive/MyDrive/ALLM_Project/'+company+'.csv')\n"
      ],
      "metadata": {
        "id": "IXN8odadUseS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(save_company_data(sc_id=[\"RI\"], page_number=1, next_page=0, years=[2017]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQkNwgcIWkSA",
        "outputId": "d3a5fc9b-b1b4-4040-cd6f-a0b0075cd2b5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "year: 2017\n",
            "page_number: 1\n",
            "next_page: 0\n",
            "url: https://www.moneycontrol.com/stocks/company_info/stock_news.php?sc_id=RI&scat=&pageno=1&next=0&durationType=Y&Year=2017&duration=1&news_type=\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "companies_data = save_company_data(sc_id=[\"RI\"], page_number=1, next_page=0, years=[2018])\n",
        "for i, df in enumerate(companies_data):\n",
        "    df.to_csv(f'company_{i+1}_data.csv', index=False)"
      ],
      "metadata": {
        "id": "ieQWZifvUybs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}