{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqgZrCf0NWUf"
      },
      "outputs": [],
      "source": [
        "!pip install lxml\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_blog_url(soup):\n",
        "  div_=soup.find_all('div', attrs={'class': 'FL PR20'})\n",
        "  url_list=[]\n",
        "  for title in div_:\n",
        "    href =title.find('a')['href']\n",
        "    url_list.append(\"https://www.moneycontrol.com/\"+href)\n",
        "  return url_list"
      ],
      "metadata": {
        "id": "vAgFXixuNaf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_blog_content(url):\n",
        "  request= requests.get(url)\n",
        "  soup =BeautifulSoup(request.text, 'lxml')\n",
        "  all_scripts= soup.find_all('script', attrs= {'type':'application/ld+json'})\n",
        "  raw_article_str=all_scripts[2].get_text().replace('\\r\\n','')\n",
        "  parts= re.split(r\"\"\"(\"[^\"]*\"|'[^']*')\"\"\", raw_article_str)\n",
        "  parts[::2] = map(lambda s: \"\".join(s.split()), parts[::2])\n",
        "  article_str=\"\".join(parts)\n",
        "  article_str= article_str[1:]\n",
        "  article_str= article_str[:-1]\n",
        "  article_dict= json.loads(article_str)\n",
        "  all_tags=soup.find_all('div', attrs= {'class':'tags_first_line'})\n",
        "  lst_all_tags=[]\n",
        "  for i in all_tags:\n",
        "    lst_all_tags.append(i.get_text())\n",
        "  tags= lst_all_tags[0].replace('TAGS:','')\n",
        "  tags=tags.replace('\\n','')\n",
        "  tags=tags.split('#')\n",
        "  tags= tags[1:]\n",
        "  tags=','.join([str(elem).strip() for elem in tags])\n",
        "  article_dict['tags']=tags\n",
        "  return article_dict"
      ],
      "metadata": {
        "id": "PdyhHnwbNhNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_page_no(url, sc_id, page_no, next, year):\n",
        "  request= requests.get(url)\n",
        "  soup =BeautifulSoup(request.text, 'lxml')\n",
        "  all_page_no =soup.find_all('div', attrs={'class': 'pages MR10 MT15'})\n",
        "  page_list =[i.text for i in all_page_no[0].find_all('a')]\n",
        "  if any(map(str.isdigit, page_list[-1])):\n",
        "\n",
        "    return int(page_list[-1]), next\n",
        "\n",
        "  else:\n",
        "    next= next + 1\n",
        "    page_no =int(page_list[-2])\n",
        "    url=\"https://www.moneycontrol.com/stocks/company_info/stock_news.php?sc_id=\"+sc_id+\"&scat=&pageno=\"+str(page_no)+\"&next=\"+str(next)+\"&durationType=Y&Year=\"+str(year)+\"&duration=1&news_type=\"\n",
        "    return get_page_no(url, sc_id, page_no, next, year)"
      ],
      "metadata": {
        "id": "MBVBgGkQNh2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_company_data(url_=\"https://www.moneycontrol.com/stocks/company_info/stock_news.php?\", sc_id=[], page_no=1, next=0, years=[]):\n",
        "  for company in sc_id:\n",
        "    df=pd.DataFrame(columns=['company', 'datePublished', 'author', 'headline', 'description', 'articleBody', 'tags', 'url'])\n",
        "    for year in years:\n",
        "      print('year:', year)\n",
        "      print('page_no:',page_no)\n",
        "      print('next:', next)\n",
        "      url=url_ + \"sc id=\"+company+\"&scat=&pageno=\"+str(page_no)+\"&next=\"+str(next)+\"&durationType=Y&Year=\"+str(year)+\"&duration=1&news_type=\"\n",
        "      print('url:', url)\n",
        "\n",
        "      max_page_no, max_next= get_page_no(url, company, page_no, next, year)\n",
        "      max_next= max_next + 1\n",
        "\n",
        "      for i in range(max_next):\n",
        "        for j in range((i*10)+1, (i*10)+11):\n",
        "          if j <= max_page_no:\n",
        "            url_list=[]\n",
        "            url=url_ + \"sc_id=\"+company+\"&scat=&pageno=\"+str(j)+\"&next=\"+str(i)+\"&durationType=Y&Year=\"+str(year)+\"&duration=1&news_type=\"\n",
        "            request= requests.get(url)\n",
        "            soup= BeautifulSoup(request.text, 'lxml')\n",
        "            url_list= get_blog_url(soup)\n",
        "\n",
        "            for url in url_list:\n",
        "              try:\n",
        "                article_dict= get_blog_content(url)\n",
        "                print(company)\n",
        "                print(article_dict['datePublished'])\n",
        "                print(article_dict['author'])\n",
        "                print(article_dict['headline'])\n",
        "                print(article_dict['description'])\n",
        "                print(article_dict['articleBody'])\n",
        "                print(article_dict['tags'])\n",
        "                print(article_dict['url'])\n",
        "                print('------------------------------------------')\n",
        "\n",
        "                article_lst =[[company,\n",
        "                              article_dict['datePublished'],\n",
        "                              article_dict['author'],\n",
        "                              article_dict['headline'],\n",
        "                              article_dict['description'],\n",
        "                              article_dict['articleBody'],\n",
        "                              article_dict['tags'], url]]\n",
        "\n",
        "\n",
        "                df = pd.concat([df, pd.DataFrame(article_lst, columns=['company', 'datePublished', 'author', 'headline', 'description', 'articleBody', 'tags', 'url'])], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "              except:\n",
        "\n",
        "                   article_lst =[[company, 'error', 'error', 'error', 'error', 'error', 'error', url]]\n",
        "\n",
        "                   df = pd.concat([df, pd.DataFrame(article_lst, columns=['company', 'datePublished', 'author', 'headline', 'description', 'articleBody', 'tags', 'url'])], ignore_index=True)\n",
        "\n",
        "                   continue\n",
        "          else:\n",
        "                 break\n",
        "  df.to_csv('/content/drive/MyDrive/ALLM_Project/'+company+'.csv')"
      ],
      "metadata": {
        "id": "znSF9Ie4NpvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(save_company_data(sc_id=[\"RI\"], page_no=1, next=0, years=[2017]))"
      ],
      "metadata": {
        "id": "BRbeCql5Ns6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc_id=\"RI\"\n",
        "page_no=1\n",
        "next=0\n",
        "year=2017\n",
        "url=\"https://www.moneycontrol.com/stocks/company_info/stock_news.php?sc_id=\"+sc_id+\"&scat=&pageno=\"+str(page_no)+\"&next=\"+str(next)+\"&durationType=Y&Year=\"+str(year)+\"&duration=1&news_type=\"\n",
        "value4=get_page_no(url, sc_id, page_no, next, year)"
      ],
      "metadata": {
        "id": "rIj0jcErO1BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(value4)"
      ],
      "metadata": {
        "id": "kVULv8KYO1vQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}